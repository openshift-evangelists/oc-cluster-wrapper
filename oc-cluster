#!/bin/bash

DOC="https://github.com/openshift-evangelists/oc-cluster-wrapper/blob/master/README.adoc"
SCRIPT_NAME=`basename "$0"`
# Helpers, maybe worth in another file
function which-ip { /sbin/ifconfig $1 | grep "inet " | awk -F: '{print $1}' | awk '{print $2}'; }
#

# Handle source locations that might be a symlink (ref: http://bit.ly/2kcvSCS)
SOURCE="${BASH_SOURCE[0]}"
while [ -h "$SOURCE" ]; do # resolve $SOURCE until the file is no longer a symlink
  DIR="$( cd -P "$( dirname "$SOURCE" )" && pwd )"
  SOURCE="$(readlink "$SOURCE")"
  [[ $SOURCE != /* ]] && SOURCE="$DIR/$SOURCE" # if $SOURCE was a relative symlink, we need to resolve it relative to the path where the symlink file was located
done
DIR="$( cd -P "$( dirname "$SOURCE" )" && pwd )"

__PLATFORM='unknown'
__UNAMESTR=$(uname)
if [[ "$__UNAMESTR" == 'Linux' ]]; then
   __PLATFORM='linux'
elif [[ "$__UNAMESTR" == 'Darwin' ]]; then
   __PLATFORM='macosx'
fi

# Platform unknown
if [[ "$__PLATFORM" == "unknown" ]]; then
  echo "Unknown platform: script should exit" && exit 1
fi

# Platform linux
if [[ "$__PLATFORM" == "linux" ]]; then
  echo "Performing some customization for platform $__PLATFORM"
  __DOCKER0_IP=$(which-ip docker0)
  echo "Using Docker0 ($__DOCKER0_IP) ip as external cluster and router address"
  OC_CLUSTER_PUBLIC_HOSTNAME=${OC_CLUSTER_PUBLIC_HOSTNAME:-${__DOCKER0_IP}}
  OC_CLUSTER_ROUTING_SUFFIX=apps.${OC_CLUSTER_PUBLIC_HOSTNAME:-${__DOCKER0_IP}}.xip.io
  EXTRA_OPTS="--forward-ports=false"
fi

USER="$(id -u)"
GROUP="$(id -g)"

OC_CLUSTER_PUBLIC_HOSTNAME=${OC_CLUSTER_PUBLIC_HOSTNAME:-127.0.0.1}
OC_CLUSTER_ROUTING_SUFFIX=${OC_CLUSTER_ROUTING_SUFFIX:-${OC_CLUSTER_PUBLIC_HOSTNAME}.xip.io}
OPENSHIFT_HOME_DIR=${OPENSHIFT_HOME:-$HOME/.oc}
OPENSHIFT_PROFILES_DIR=${OPENSHIFT_PROFILES_DIR:-$OPENSHIFT_HOME_DIR/profiles}
PLUGINS_DIR=${PLUGINS_DIR:-$DIR/plugins.d}
OC_CLUSTER_PREFILL_PVS=${OC_CLUSTER_PREFILL_PVS:-10}
OC_CLUSTER_TZ=${OC_CLUSTER_TZ:-$(date +%Z)}

OS_DEFAULT_USER="developer"
OS_DEFAULT_PROJECT="myproject"

##################
# Add common functions

#  markRestart: Marks that a restart will be required after the provisioning
#  forceRestart: Will execute a restart and unmark a required restart
#  restartIfRequire
#
__RESTART=0


function markRestart {
   # Just set a restart flag
   __RESTART=1
}

function forceRestart {
   echo -n "Restarting openshift. "
   docker stop origin &> /dev/null && docker start origin &> /dev/null
   echo "Done"
   # TODO: Check that restarted succesfully
   # Clear restart flag
   __RESTART=0
}

function activeProfile {
   local _active_profile=$([ -f $OPENSHIFT_HOME_DIR/active_profile ] && cat $OPENSHIFT_HOME_DIR/active_profile)
   echo "$_active_profile"
}

function profileDir {
   local _active_profile=$(activeProfile)
   if [ "$_active_profile" != "" ]
   then
      echo "$OPENSHIFT_PROFILES_DIR/$_active_profile"
   else
      echo ""
   fi
}

function masterConfigDir {
   local _active_profile=$(activeProfile)
   if [ "$_active_profile" != "" ]
   then
      echo "$OPENSHIFT_PROFILES_DIR/$_active_profile/config/master"
   else
      echo ""
   fi
}

function internalEtcdDir {
   echo "/var/lib/origin/openshift.local.etcd"
}

function internalProfileDir {
   echo "/var/lib/origin/openshift.local.config"
}

function internalMasterConfigDir {
   echo "/var/lib/origin/openshift.local.config/master"
}

function isClusterRunning {
   set -x
   [[ "$(docker ps -f name=origin -q)" == "" ]] && return 1
   return 0
}

function configPatch {
   echo "Applying this path to the config: [$@]"

   # TODO: Make temp file random
   cat <<-EOF > /tmp/patch_00001
   /usr/bin/cp $(internalMasterConfigDir)/master-config.yaml $(internalMasterConfigDir)/master-config.orig.yaml

   /usr/bin/openshift ex config patch $(internalMasterConfigDir)/master-config.orig.yaml --patch="$@" > $(internalMasterConfigDir)/master-config.yaml

   # TODO: Verify non empty contents or revert saved file
   if [ ! -s $(internalMasterConfigDir)/master-config.yaml ]; then
      /usr/bin/cp $(internalMasterConfigDir)/master-config.orig.yaml $(internalMasterConfigDir)/master-config.yaml
      echo "No change applied"
   else
      echo "Configuration change applied"
   fi
EOF
   chmod 755 /tmp/patch_00001

   docker cp /tmp/patch_00001 origin:/tmp/patch_00001
   docker exec -t origin /usr/bin/bash /tmp/patch_00001
   rm /tmp/patch_00001
}

function execInTheContainer {
   echo "Executing this in the container: [$@]"

   cat <<-EOF > /tmp/patch_00002
   $@
EOF
   chmod 755 /tmp/patch_00002

   docker cp /tmp/patch_00002 origin:/tmp/patch_00002
   docker exec -t origin /usr/bin/bash /tmp/patch_00002
   # rm /tmp/patch_00001
}

function error_exit() {
  echo -e "$@"
  exit 1
}

function domainSuffix {
   local _suffix=$([ -f $(profileDir)/suffix ] && cat $(profileDir)/suffix)
   echo "$_suffix"
}

function bindIP {
   local _hostname=$([ -f $(profileDir)/hostname ] && cat $(profileDir)/hostname)
   echo "$_hostname"
}

# Function that tells me if the client used supports some features
#
# pv,proxy,image-streams
#
# Return 0 if it supports it, non zero else
function supports {
  if [ "$1" == "pv" ]; then
    ret=$(oc cluster up -h | grep host-pv-dir | wc -l)
    [ $ret -eq 0 ] && return 1 || return 0
  elif [ "$1" == "proxy" ]; then
    ret=$(oc cluster up -h | grep proxy | wc -l)
    [ $ret -eq 0 ] && return 1 || return 0
  elif [ "$1" == "image-streams" ]; then
    ret=$(oc cluster up -h | grep image-streams | wc -l)
    [ $ret -eq 0 ] && return 1 || return 0
  fi
}

##################
#
#
#
#
function up-dev {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  # Execute the dev stuff
  local IP=$(openshift start --print-ip 2> /dev/null)

  [ -z $IP ] && echo "OpenShift development binary can not be properly started" && return 0

  docker run --privileged --net=host -v /var/run/docker.sock:/var/run/docker.sock -d --name=bindmountproxy cewong/bindmountproxy proxy ${IP}:2375 $(which openshift)
  sleep 2
  local docker_host=tcp://${IP}:2375
  DEV_DOCKER_HOST=${docker_host}

  # TODO: Add a function to mark the cluster as dev
  # Call up
  up "$*"
}

function up {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  local _profile="$1"

  # Test that there is not a running cluster already
  status &> /dev/null && error_exit "There is a cluster already running. You can not run 2 cluster at the same time"

  if [ "$_profile" == "" ] || [[ $_profile == -* ]]
  then
     echo "Using default profile"
    _profile="default"
  else
    shift # Remove profile name
  fi

  # TODO: If the cluster is already created, do not provision stuff
  if [ ! -d "$OPENSHIFT_PROFILES_DIR/$_profile" ] || [ ! -e "$OPENSHIFT_PROFILES_DIR/$_profile/run" ]
  then
    echo "[INFO] Running a new cluster"
    # This is where oc cluster stores it's data
    local OPENSHIFT_HOST_DATA_DIR=$OPENSHIFT_PROFILES_DIR/$_profile/data
    local OPENSHIFT_HOST_CONFIG_DIR=$OPENSHIFT_PROFILES_DIR/$_profile/config
    local OPENSHIFT_HOST_VOLUMES_DIR=$OPENSHIFT_PROFILES_DIR/$_profile/volumes
    local OPENSHIFT_HOST_PLUGINS_DIR=$OPENSHIFT_PROFILES_DIR/$_profile/plugins
    local OPENSHIFT_HOST_PV_DIR=$OPENSHIFT_PROFILES_DIR/$_profile/pv

    mkdir -p $OPENSHIFT_PROFILES_DIR/$_profile
    mkdir -p $OPENSHIFT_HOST_CONFIG_DIR
    mkdir -p $OPENSHIFT_HOST_DATA_DIR
    mkdir -p $OPENSHIFT_HOST_VOLUMES_DIR
    mkdir -p $OPENSHIFT_HOST_PLUGINS_DIR
    mkdir -p $OPENSHIFT_HOST_PV_DIR

    # Save hostname and suffix
    echo "$OC_CLUSTER_ROUTING_SUFFIX"  > $OPENSHIFT_PROFILES_DIR/$_profile/suffix
    echo "$OC_CLUSTER_PUBLIC_HOSTNAME" > $OPENSHIFT_PROFILES_DIR/$_profile/hostname
    echo "$DEV_DOCKER_HOST" > $OPENSHIFT_PROFILES_DIR/$_profile/dev

    # Build command line
    local CMDLINE="oc cluster up"
    CMDLINE+=" --public-hostname $OC_CLUSTER_PUBLIC_HOSTNAME"
    [ ! -z $OC_CLUSTER_ROUTING_SUFFIX ] && CMDLINE+=" --routing-suffix $OC_CLUSTER_ROUTING_SUFFIX"
    CMDLINE+=" --host-data-dir $OPENSHIFT_HOST_DATA_DIR"
    CMDLINE+=" --host-config-dir $OPENSHIFT_HOST_CONFIG_DIR"
    if supports pv; then
      CMDLINE+=" --host-pv-dir $OPENSHIFT_HOST_PV_DIR"
    fi
    CMDLINE+=" --use-existing-config"
    CMDLINE+=" -e TZ=$OC_CLUSTER_TZ"
    # TODO: If --version not specified, detect client version and use that one
    # TODO: If --registry not specified, detect client version and use that one
    CMDLINE+=" $@"
    # TODO: Add support for proxies and image streams, and
    # custom arguments. Do not support anything coming from cluster up

    echo "$CMDLINE" > $OPENSHIFT_PROFILES_DIR/$_profile/run
    echo "$(<$OPENSHIFT_PROFILES_DIR/$_profile/run)"
    if [ ! -z $DEV_DOCKER_HOST ]; then
      echo "Running in developer mode, using DOCKER_HOST=$DEV_DOCKER_HOST"
      DOCKER_HOST=$DEV_DOCKER_HOST $CMDLINE -e DOCKER_HOST=$DEV_DOCKER_HOST
    else
      $CMDLINE
    fi

    # Fix permissions in linux
    if [[ "$__PLATFORM" == "linux" ]]; then
      docker exec origin chown -R $USER:$GROUP $(internalProfileDir) $(internalProfileDir)
      echo "-- Permissions on profile dir fixed"
    fi

    status &> /dev/null  || cleanupClusterAndExit $_profile
    # Create the profile markfile
    echo "${_profile}" > $OPENSHIFT_HOME_DIR/active_profile

    CONTEXT="default/"$(echo $OC_CLUSTER_PUBLIC_HOSTNAME| tr -s '.' '-')":8443/system:admin"
    #Add developer as sudoer
    oc adm policy add-cluster-role-to-group sudoer system:authenticated \
        --config="${OPENSHIFT_HOST_CONFIG_DIR}/master/admin.kubeconfig" \
        --context="$CONTEXT" &> /dev/null
    echo "-- Any user is sudoer. They can execute commands with '--as=system:admin'"

    # Prefill with 10 volumes
    for i in $(seq -f %02g 1 $OC_CLUSTER_PREFILL_PVS)
    do
      create-volume vol${i} &> /dev/null
    done
    echo "-- $OC_CLUSTER_PREFILL_PVS Persistent Volumes are available for use"

    # TODO: OpenIssue and wait for it to be fixed and remove
    # Hack for metrics not properly working
    oc adm policy add-role-to-user view system:serviceaccount:openshift-infra:hawkular -n openshift-infra --as=system:admin &> /dev/null

    # Create user admin that can log into the console
    oc adm policy add-cluster-role-to-user cluster-admin admin --as=system:admin &> /dev/null
    echo "-- User admin has been set as cluster administrator"

    # Create the context named after the profile to allow for reuse
    # Info from: http://kubernetes.io/docs/user-guide/kubeconfig-file/
    # TODO: Change skip-tls with the certs --certificate-authority=
    oc adm config set-cluster ${_profile} --server=https://$OC_CLUSTER_PUBLIC_HOSTNAME:8443 --insecure-skip-tls-verify=true &> /dev/null
    # TODO: Change token for secure certs: --client-certificate=path/to/my/client/cert --client-key=path/to/my/client/key
    oc adm config set-credentials ${OS_DEFAULT_USER}@${_profile} --token=$(oc whoami -t) &> /dev/null
    oc adm config set-context ${_profile} --cluster=${_profile} --user=${OS_DEFAULT_USER}@${_profile} --namespace=${OS_DEFAULT_PROJECT} &> /dev/null
    oc adm config use-context ${_profile} # &> /dev/null

    # Add a label to the created images so that can be removed with the cluster
    echo "-- Adding an oc-profile=${_profile} label to every generated image so they can be later removed"
    # TODO: Add oc-version once I set up the versions
    configPatch "{\\\"admissionConfig\\\": {\\\"pluginConfig\\\": {\\\"BuildDefaults\\\": {\\\"configuration\\\": {\\\"apiVersion\\\": \\\"v1\\\",\\\"kind\\\": \\\"BuildDefaultsConfig\\\",\\\"imageLabels\\\": [{\\\"name\\\": \\\"oc-profile\\\",\\\"value\\\": \\\"${_profile}\\\"}]}}}}}" &> /dev/null
    markRestart
    echo "[INFO] Cluster created succesfully"
  else
    echo "[INFO] Running a previously created cluster"
    #TODO: Verify that charateristics that the command line the cluster created the profile are valid, like creating a cluster with 1.5 and pv and then downgrading the client to 1.4 and not starting again.
    # Just start the cluster
    echo "$(<$OPENSHIFT_PROFILES_DIR/$_profile/run) $@"
    . $OPENSHIFT_PROFILES_DIR/$_profile/run "$@"
    status &> /dev/null || error_exit "[ERROR] Cluster has not started correctly. Profile configuration will be preserved"
    # Create the profile markfile
    echo "${_profile}" > $OPENSHIFT_HOME_DIR/active_profile

    # Use right context after starting up (Similar to log in)
    oc adm config use-context ${_profile} # &> /dev/null
  fi
}

function up.help {
  echo "Starts/Creates a cluster with the given profile name (or default)"
  echo "in case no profile is specified"
  echo ""
  echo "Profile information will be stored in $OPENSHIFT_PROFILES_DIR/<PROFILE>"
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME up [<profile>] [<args>]"
  echo ""
  echo "Available Environement variables:"
  echo ""
  echo "OC_CLUSTER_PUBLIC_HOSTNAME (defaults to 127.0.0.1)"
  echo "OC_CLUSTER_ROUTING_SUFFIX (defaults to apps.127.0.0.1.xip.ip)"
  echo ""
  echo "Available Arguments:"
  echo ""
  echo "--version"
  echo "--type='origin': Specify the type of install ocp|origin"
  echo "--image-streams='centos7': Specify which image streams to use, centos7|rhel7"
  echo "--logging=false: If true, install logging (experimental)"
  echo "--metrics=false: If true, install metrics (experimental)"
  echo "--http-proxy='': HTTP proxy to use for master and builds"
  echo "--https-proxy='': HTTPS proxy to use for master and builds"
  echo "--no-proxy=[]: List of hosts or subnets for which a proxy should not be used"
  echo "-e, --env=[]: Specify a key-value pair for an environment variable to set on OpenShift container"
  echo ""
  echo "See the documentation at $DOC"
}

function cleanupClusterAndExit() {
  [ "$1" != "" ] && rm -rf $OPENSHIFT_PROFILES_DIR/$1
  error_exit "[ERROR] There's been an error creating the cluster, the profile [$1] will be removed"
}

function down {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  status &> /dev/null || return
  echo "Bringing the cluster down"
  oc cluster down
  # TODO: Check if the cluster is DEV cluster
  docker rm -f bindmountproxy 2&> /dev/null
  rm -f $OPENSHIFT_HOME_DIR/active_profile
}

function down.help {
  echo "Stops the current active cluster. No information will be removed"
  echo "so the cluster can be later started again"
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME down"
  echo ""
  echo "See the documentation at $DOC"
}

#
# Args:
#  $1: [-s] silent
function status {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  [[ "$(docker ps -f name=origin -q)" == "" ]] && echo "no cluster running" && return 1
  echo "oc cluster running. Current profile <$([ -f $OPENSHIFT_HOME_DIR/active_profile ] && cat $OPENSHIFT_HOME_DIR/active_profile || echo 'unknown')>"
  oc cluster status
  return 0
}

function status.help {
  echo "Displays status information about the active cluster/profile"
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME status"
  echo ""
  echo "See the documentation at $DOC"
}

# If the cluster is up, it will bring it down and destroy active_profile
# Otherwise will ask for profile
function destroy {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  local _profile=$1
  local _active_profile=$([ -f $OPENSHIFT_HOME_DIR/active_profile ] && cat $OPENSHIFT_HOME_DIR/active_profile || echo '')

  if [ $# -lt 1 ] && [ "${_active_profile}" == "" ]
  then
    error_exit "You need to specify a profile, or have a running cluster"
  fi
  [ "$_profile" == "" ] && _profile=$_active_profile

  if [ ! -d "$OPENSHIFT_PROFILES_DIR/$_profile" ]
  then
    error_exit "There is no profile named $_profile"
  fi

  if [ "$_profile" != "" ]
  then
     read -p "Are you sure you want to destroy cluster with profile <$_profile> (y/n)? " -n 1 -r
     echo    # move to a new line
     if [[ $REPLY =~ ^[Yy]$ ]]
     then
        # do dangerous stuff
        echo "Removing profile $_profile"
        if [ "$_active_profile" == "$_profile" ]
        then
           down
        fi
        echo "Removing $OPENSHIFT_PROFILES_DIR/$_profile"
        [ "$_profile" != "" ] && rm -rf $OPENSHIFT_PROFILES_DIR/$_profile
        echo "Removing .kubeconfig profiles"
        oc adm config delete-cluster $_profile &> /dev/null
        oc adm config delete-context $_profile &> /dev/null
        # TODO: Remove the images
        local _num_images=$(docker images --filter label=oc-profile=${_profile} -q | wc -l)
        echo "Removing $((${_num_images})) images built with this cluster"
        docker rmi $(docker images --filter label=oc-profile=${_profile} -q) &> /dev/null
     fi
  fi
}

function destroy.help {
  echo "Destroys the specified cluster, or the current active cluster"
  echo "if none is specified."
  echo ""
  echo "All the information store for the profile will be removed"
  echo "- This information is stored in $OPENSHIFT_PROFILES_DIR/<PROFILE>"
  echo "- Also the profile will be cleared from the .kubeconfig file"
  echo "- All the images created by this cluster will be removed"
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME destroy [<PROFILE>]"
  echo ""
  echo "See the documentation at $DOC"
}

function list {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  echo "Profiles:"
  [[ ! -d "$OPENSHIFT_PROFILES_DIR/" ]] && error_exit "[ERROR] Missing profiles dir [$OPENSHIFT_PROFILES_DIR]"
  for i in `ls $OPENSHIFT_PROFILES_DIR`
  do
    echo "- $i"
  done
}

function list.help {
  echo "List all the available profiles"
  echo ""
  echo "Profiles are located in $OPENSHIFT_PROFILES_DIR"
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME list"
  echo ""
  echo "See the documentation at $DOC"
}

function ssh {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  echo "Going into the Origin Container"
  docker exec -it origin /bin/bash
}

function ssh.help {
  echo "Connects to the origin container."
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME ssh"
  echo ""
  echo "See the documentation at $DOC"
}

function console {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  [ ! -z "$(bindIP)" ] &&
    echo "https://$(bindIP):8443/console" ||
    echo "No console"
}

function console.help {
  echo "Prints a url link to the cluster's console"
  echo ""
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME console"
  echo ""
  echo "See the documentation at $DOC"
}

function completion() {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  local shell=$1
  [[ $shell != "bash" ]] && echo "Only bash supported for now" && return
cat << "EOF"
OPENSHIFT_HOME_DIR=${OPENSHIFT_HOME:-$HOME/.oc}
_oc_cluster_completion() {
  local cur prev command commands profiles cluster_args
  COMPREPLY=()   # Array variable storing the possible completions.
  # TODO: Add "-h --help"
  cur=${COMP_WORDS[COMP_CWORD]}
  prev="${COMP_WORDS[COMP_CWORD-1]}"
  command="${COMP_WORDS[1]}"
  commands="up down destroy list status ssh console plugin-list plugin-install plugin-uninstall -h --help"

  profiles=$(ls -d $OPENSHIFT_PROFILES_DIR/*/ | xargs -L1 basename)

  boolean_args="--create-machine= --forward-ports= --metrics= --skip-registry-check= --use-existing-config="
  cluster_args="--docker-machine= -e --env= --host-config-dir= --host-data-dir= --host-volumes-dir= --image= --public-hostname= --routing-suffix= --server-loglevel= --version= $boolean_args"
  #todo complete these args more
  [[ "$command" == "up" && $COMP_CWORD -gt 2 ]] && \
    COMPREPLY=( $( compgen -W "$cluster_args" -- $cur ) ) && \
    return 0

  if [[ ${cur} == -* || ${COMP_CWORD} -eq 1 ]] ; then
    COMPREPLY=( $( compgen -W "$commands" -- $cur ) )
    return 0
  fi

  case "$prev" in
    up|destroy)
      COMPREPLY=( $( compgen -W "$profiles" -- $cur ) )
      ;;
  esac
}
complete -o nospace -F _oc_cluster_completion oc-cluster
EOF
}

function completion.help {
  echo "Provides bash command line completion for core $SCRIPT_NAME"
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME completion bash"
  echo ""
  echo "See the documentation at $DOC"
}

#
# Install plugin
#
function plugin-install {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  local _plugin=$1
  [ ! -e $PLUGINS_DIR/${_plugin}.local.plugin ] && error_exit "[ERROR] There's no plugin named [${_plugin}]"
  # A plugin can only be installed on an active cluster
  if [ "$(activeProfile)" != "" ]; then
     echo "Install $_plugin"
     # Make sure plugins dir exists (legacy)
     mkdir -p $(profileDir)/plugins
     # Symlink
     ln -s $PLUGINS_DIR/${_plugin}.local.plugin $(profileDir)/plugins &> /dev/null
     # Execute install
     source $(profileDir)/plugins/${_plugin}.local.plugin
     echo "========"
     ${_plugin}.install
     # TODO: Check that the plugin has exited with 0, else print an error and unlink
     echo "========"
     echo "Installed"
  else
    echo "[INFO] A plugin can not be installed if there's no active cluster"
  fi
}

function plugin-install.help {
  echo "Installs a local plugin"
  echo ""
  echo "This will link the plugin in $OPENSHIFT_PROFILES_DIR/<PROFILE>/plugins"
  echo "and make all the provided command by the plugin also available."
  echo "After the plugin is installed, executing $SCRIPT_NAME -h "
  echo "will list the new commands"
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME plugin-install <PLUGIN-NAME>"
  echo ""
  echo "See the documentation at $DOC"
}

function plugin-uninstall {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

   local _plugin=$1
   [ ! -e $PLUGINS_DIR/${_plugin}.local.plugin ] && error_exit "[ERROR] There's no plugin named [${_plugin}]"
   # A plugin can only be installed on an active cluster
   if [ "$(activeProfile)" != "" ]; then
      echo "Uninstall $_plugin"
      # Make sure plugins dir exists (legacy)
      mkdir -p $(profileDir)/plugins
      # Symlink
      ln -s $PLUGINS_DIR/${_plugin}.local.plugin $(profileDir)/plugins &> /dev/null
      # Execute install
      source $(profileDir)/plugins/${_plugin}.local.plugin
      echo "========"
      ${_plugin}.uninstall
      # TODO: Check that the plugin has exited with 0, else print an error and unlink
      echo "========"
      echo "Uninstalled"
   else
     echo "[INFO] A plugin can not be uninstalled if there's no active cluster"
   fi
}

function plugin-uninstall.help {
  echo "Uninstall a local plugin"
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME plugin-uninstall <PLUGIN-NAME>"
  echo ""
  echo "See the documentation at $DOC"
}

function plugin-list {
  [ "$1" == "-h" ] || [ "$1" == "--help" ] && ${FUNCNAME[0]}.help && return 0

  # List the global plugins and then the local plugins, marking which ones are installed
  [[ ! -d "$PLUGINS_DIR" ]] && error_exit error_exit "[ERROR] Plugins directory is missing [$PLUGINS_DIR]. Have you installed the script properly?"
  for i in `ls $PLUGINS_DIR/*.local.plugin`
  do
    local _filename=$(basename "$i")
    local _script=${_filename%%.*}
    echo "========"
    echo "Plugin: ${_script}"
    if [ "$(activeProfile)" != "" ]; then
      [ -L $(profileDir)/plugins/${_filename} ] && echo "(Installed)"
    fi
    . ${i}
    echo ""
   done
}

function plugin-list.help {
  echo "List all the available plugins."
  echo ""
  echo "Plugins are locate in $PLUGINS_DIR and can be local or global"
  echo " - Global plugins are always available"
  echo " - Local plugins need to be installed via plugin-install command"
  echo ""
  echo "Usage:"
  echo " $SCRIPT_NAME plugin-list"
  echo ""
  echo "See the documentation at $DOC"
}

function help {
  echo "Usage:"
  echo " $SCRIPT_NAME -h"
  echo ""
  echo "Available Commands:"
  echo ""
  echo "  oc-cluster up [profile] [OPTIONS]"
  echo "  oc-cluster down"
  echo "  oc-cluster destroy [profile]"
  echo "  oc-cluster list"
  echo "  oc-cluster status"
  echo "  oc-cluster ssh"
  echo "  oc-cluster console"
  echo "  oc-cluster completion bash"
  echo "  oc-cluster plugin-install <plugin>"
  echo "  oc-cluster plugin-uninstall <plugin>"
  echo "  oc-cluster plugin-list"
  # Help for global plugins
  [[ ! -d "$PLUGINS_DIR" ]] && error_exit "[ERROR] Plugins directory is missing [$PLUGINS_DIR]. Have you installed the script properly?"
  for i in $PLUGINS_DIR/*.global.plugin
  do
    local _filename=$(basename "$i")
    local _script=${_filename%%.*}
    ${_script}.help
  done
  # Help for installed plugins in the active profile
  if [ "$(activeProfile)" != "" ]; then
    for i in $(find $(profileDir)/plugins -type f -name \*.local.plugin)
    do
      local _filename=$(basename "$i")
      local _script=${_filename%%.*}
      ${_script}.help
    done
  fi
  echo ""
  echo ""
  echo "See the documentation at $DOC"
}

#
#
#
# Load the plugins
for plugin in $DIR/plugins.d/*.global.plugin; do
  source  $plugin
done
# If there is no active profile, there's no local plugins to load
if [ "$(activeProfile)" != "" ]; then
  # Make sure plugins dir exists (legacy)
  mkdir -p $(profileDir)/plugins
  for plugin in `ls $(profileDir)/plugins/*.local.plugin &> /dev/null`; do
    source $plugin
  done
fi

# Use -gt 1 to consume two arguments per pass in the loop (e.g. each
# argument has a corresponding value to go with it).
# Use -gt 0 to consume one or more arguments per pass in the loop (e.g.
# some arguments don't have a corresponding value to go with it such
# as in the --default example).
if [[ $# -gt 0 ]]
then
   key="$1"
   case $key in
      up-dev)
       shift # past argument
       up-dev "$@"
       ;;
      up)
        shift # past argument
        up "$@"
        ;;
      down)
        shift # past argument
        down "$@"
        ;;
      destroy)
        shift # past argument
        destroy "$@"
        ;;
      list)
        list
        ;;
      status)
        status
        ;;
      ssh)
        shift
        ssh "$@"
        ;;
      console)
        shift
        console "$@"
        ;;
      completion)
        shift
        completion "$@"
        ;;
      plugins-list)
         plugins-list
         ;;
      -h|--help)
        help
        ;;
      --*)
        shift
        ${key:2}.execute  "$@"
        ;;
      *)
        shift # past argument
        $key "$@" 2> /dev/null || echo "Command $key not found"
        ;;
   esac
else
   help
fi

# After execution of a plugin, we need to know if a restart is required
[ $__RESTART -eq 1 ] && forceRestart
